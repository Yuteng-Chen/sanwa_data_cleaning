"""
3B/7Bæ¨¡å‹æ•°æ®æ¸…ç†ç®¡é“ - å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹
Data Cleaning Pipeline - Full Automation
é’ˆå¯¹ 2025-12-22 æ‰¹æ¬¡ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import os
import sys
import json
import glob
import shutil
import re
import cv2
from pathlib import Path
from datetime import datetime
import concurrent.futures
import threading
from collections import defaultdict

# å…³é”®ï¼šå¯¼å…¥ ollama åº“
import ollama 

# å¯¼å…¥é…ç½® (ç¡®ä¿ config_pipeline.py åœ¨åŒä¸€ç›®å½•ä¸‹)
try:
    from config_pipeline import *
except ImportError:
    print("âŒ Critical Error: 'config_pipeline.py' not found!")
    sys.exit(1)

print_lock = threading.Lock()

# ================= é˜¶æ®µ1: æ•°æ®éªŒè¯å’Œæ¸…ç† =================
class DataValidator:
    """æ•°æ®éªŒè¯å™¨ - æ£€æµ‹å¼‚å¸¸å€¼"""
    
    def __init__(self, max_decimals=3, outlier_threshold=5.0, z_score_threshold=3.0):
        self.max_decimals = max_decimals
        self.outlier_threshold = outlier_threshold
        self.z_score_threshold = z_score_threshold
    
    def validate_value(self, val, data_type):
        """éªŒè¯å•ä¸ªå€¼"""
        val_str = str(val).strip()
        if pd.isna(val) or val_str == '' or val_str.lower() == 'nan':
            return False, val, "Empty/NaN"
        
        if data_type == 'STATUS':
            val_upper = val_str.upper()
            if val_upper.startswith('O') or val_upper == '0' or 'OK' in val_upper:
                return True, 'OK', None
            if val_upper.startswith('N'):
                return True, 'NG', None
            if val_upper == 'K':
                return True, 'OK', None
            if val_upper in ['', 'NAN', 'NA', 'NULL', 'NONE']:
                return False, val, "Empty/Invalid Status"
            return False, val, "Unknown Status"
        
        elif data_type == 'INTEGER':
            try:
                return True, int(float(val_str)), None
            except:
                pass
            clean_val = re.sub(r'[^\d-]', '', val_str)
            if re.match(r'^-?\d+$', clean_val):
                return True, int(clean_val), None
            return False, val, "Not an Integer"
        
        elif data_type == 'FLOAT':
            if re.match(r'^-?\d+(\.\d+)?$', val_str):
                if '.' in val_str and len(val_str.split('.')[1]) > self.max_decimals:
                    return False, val, f"Suspicious Pattern (>{self.max_decimals} decimals)"
                try:
                    return True, float(val_str), None
                except:
                    pass
            return False, val, "Invalid Float"
        
        elif data_type == 'TIME':
            if re.match(r'^\d{1,2}:\d{2}:\d{2}$', val_str):
                return True, val_str, None
            return False, val, "Invalid Time"
        
        return False, val, "Unknown Type"
    
    def detect_outliers(self, series, data_type):
        """ç»Ÿè®¡å¼‚å¸¸å€¼æ£€æµ‹ (Ratio + Z-Score)"""
        if data_type not in ['FLOAT', 'INTEGER']:
            return []
        
        nums = pd.to_numeric(series, errors='coerce').dropna()
        if len(nums) < 5:
            return []
        
        median = nums.median()
        mean = nums.mean()
        std = nums.std()
        
        outlier_results = []
        
        for idx, val in series.items():
            try:
                val_float = float(val)
                if val_float == 0 or pd.isna(val_float):
                    continue
                
                # Method 1: Ratio (é’ˆå¯¹æ¼å°æ•°ç‚¹)
                if median != 0:
                    ratio = val_float / median
                    if ratio > self.outlier_threshold or ratio < (1.0 / self.outlier_threshold):
                        outlier_results.append((idx, "Statistical Outlier (Likely Missing Decimal)"))
                        continue
                
                # Method 2: Z-Score (é’ˆå¯¹åç¦»å€¼)
                if std > 0:
                    z_score = abs((val_float - mean) / std)
                    if z_score > self.z_score_threshold:
                        outlier_results.append((idx, f"Z-Score Outlier (Z={z_score:.2f})"))
            except:
                pass
        return outlier_results

class Stage1_DataCleaning:
    """é˜¶æ®µ1: æ•°æ®æ¸…ç†"""
    def __init__(self, input_dir, output_dir, crops_base):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.crops_base = Path(crops_base)
        self.validator = DataValidator(MAX_DECIMALS, OUTLIER_THRESHOLD, Z_SCORE_THRESHOLD)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def get_config_for_file(self, df):
        for config in ROI_CONFIGS:
            if config['Trigger_Col'] in df.columns:
                return config
        return None
    
    def copy_crop_for_review(self, csv_base_name, filename, roi_id, dest_folder):
        """å¤åˆ¶å¼‚å¸¸å›¾ç‰‡ï¼Œæ”¯æŒå¤šç§è·¯å¾„ç»“æ„"""
        try:
            folder_name = os.path.splitext(filename)[0]
            # æœç´¢è·¯å¾„ç­–ç•¥ (é€‚é…ä¸åŒçš„æˆªå›¾ç›®å½•ç»“æ„)
            potential_paths = [
                self.crops_base / csv_base_name / folder_name / f"{roi_id}.jpg",
                self.crops_base / csv_base_name / folder_name / f"{roi_id}.png",
                self.crops_base / folder_name / f"{roi_id}.jpg",
                self.crops_base / folder_name / f"{roi_id}.png",
            ]
            
            src_file = None
            for p in potential_paths:
                if p.exists():
                    src_file = p
                    break
            
            if not src_file:
                return False
            
            target_folder = dest_folder / folder_name
            target_folder.mkdir(parents=True, exist_ok=True)
            shutil.copy(src_file, target_folder / src_file.name)
            return True
        except:
            return False

    def process_single_csv(self, csv_path):
        filename = csv_path.name
        base_name = csv_path.stem
        print(f"\nğŸ“„ Processing: {filename}...")
        
        # æ–­ç‚¹ç»­ä¼ : æ£€æŸ¥æ˜¯å¦å·²å¤„ç†å®Œæˆ
        output_cleaned = self.output_dir / f"{base_name}_Cleaned.csv"
        if output_cleaned.exists():
            print(f"  â­ï¸  Skipped (already processed: {output_cleaned.name})")
            return
        
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            print(f"  âŒ Error reading CSV: {e}")
            return
        
        config = self.get_config_for_file(df)
        if not config:
            print(f"  âš ï¸  Skipped: Unknown CSV format (Column mismatch)")
            return
        
        roi_map = config['Columns']
        if 'Filename' in df.columns:
            df.sort_values(by='Filename', inplace=True)
        
        df_clean = df.copy()
        abnormal_records = []
        
        # 1. æ ¼å¼éªŒè¯
        for idx, row in df.iterrows():
            for roi_col, dtype in roi_map.items():
                if roi_col in df.columns:
                    val = row[roi_col]
                    is_valid, clean_val, reason = self.validator.validate_value(val, dtype)
                    if is_valid:
                        df_clean.at[idx, roi_col] = clean_val
                    else:
                        abnormal_records.append({
                            'Filename': row.get('Filename', 'Unknown'),
                            'ROI_ID': roi_col,
                            'Value': val,
                            'Reason': reason
                        })
        
        # 2. ç»Ÿè®¡æ£€æµ‹
        for roi_col, dtype in roi_map.items():
            if roi_col in df_clean.columns:
                outlier_results = self.validator.detect_outliers(df_clean[roi_col], dtype)
                for idx, reason in outlier_results:
                    abnormal_records.append({
                        'Filename': df_clean.at[idx, 'Filename'],
                        'ROI_ID': roi_col,
                        'Value': df_clean.at[idx, roi_col],
                        'Reason': reason
                    })
        
        # ä¿å­˜ç»“æœ
        df_clean.to_csv(self.output_dir / f"{base_name}_Cleaned.csv", index=False)
        
        if abnormal_records:
            df_abn = pd.DataFrame(abnormal_records).drop_duplicates()
            df_abn.to_csv(self.output_dir / f"{base_name}_Abnormal_Log.csv", index=False)
            
            # å¤åˆ¶å›¾ç‰‡ä¾›æ£€æŸ¥
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨äº† config_pipeline ä¸­çš„ ABNORMAL_CROPS_BASE
            crop_dest = ABNORMAL_CROPS_BASE / base_name
            crop_dest.mkdir(parents=True, exist_ok=True)
            
            count = 0
            for _, rec in df_abn.iterrows():
                if self.copy_crop_for_review(base_name, rec['Filename'], rec['ROI_ID'], crop_dest):
                    count += 1
            print(f"  âš ï¸  Found {len(df_abn)} issues. Copied {count} images for review.")
        else:
            print(f"  âœ… No issues found.")

    def run(self):
        print("\n" + "="*60)
        print("STAGE 1: Data Validation")
        print("="*60)
        csv_files = list(self.input_dir.glob("*.csv"))
        # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶
        csv_files = [f for f in csv_files if not any(x in f.name for x in ['_Cleaned', '_Log', '_Fixed'])]
        
        if not csv_files:
            print(f"âŒ No input CSV files found in {self.input_dir}")
            return
            
        print(f"Found {len(csv_files)} CSV files to process.\n")
        for f in csv_files:
            self.process_single_csv(f)

# ================= é˜¶æ®µ2: æ¨¡å‹å¼‚å¸¸ä¿®æ­£ =================
class Stage2_Correction:
    """é˜¶æ®µ2: ä½¿ç”¨ Ollama æ¨¡å‹ä¿®æ­£å¼‚å¸¸"""
    def __init__(self, cleaned_dir, crops_base, output_dir):
        self.cleaned_dir = Path(cleaned_dir)
        self.crops_base = Path(crops_base)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def calculate_medians(self, csv_path):
        """è®¡ç®—æ¯ä¸€åˆ—çš„ä¸­ä½æ•°ä½œä¸º prompt çš„å‚è€ƒ"""
        try:
            df = pd.read_csv(csv_path)
            medians = {}
            for col in df.columns:
                if not col.startswith('ROI_'): continue
                roi_type = get_roi_type(col)
                if roi_type in ['INTEGER', 'FLOAT']:
                    vals = pd.to_numeric(df[col], errors='coerce').dropna()
                    vals = vals[vals > 0]
                    if len(vals) >= 5:
                        medians[col] = vals.median()
                elif roi_type == 'STATUS':
                    vc = df[col].value_counts()
                    if not vc.empty:
                        medians[col] = vc.index[0]
            return medians
        except:
            return {}

    def clean_llm_output(self, text):
        """æ¸…ç† LLM è¿”å›çš„å¤šä½™å­—ç¬¦"""
        if not text: return "ERROR"
        # ç§»é™¤æ¨¡å‹ç‰¹æ®Štoken
        text = re.sub(r'<\|.*?\|>', '', text)
        text = text.strip().split('\n')[0].strip()
        # å°è¯•åªæå–æ•°å­—/çŠ¶æ€
        match = re.search(r'([0-9\.]+|OK|NG)', text, re.IGNORECASE)
        if match:
            return match.group(1)
        return text

    def run_inference(self, image_path, roi_id, median_val, original_val):
        """è°ƒç”¨ Ollama"""
        try:
            # ä½¿ç”¨ config_pipeline ä¸­çš„ get_prompt å‡½æ•°
            prompt = get_prompt(roi_id, 'correction', original_val, median_val)
            
            # ä½¿ç”¨ 7B æ¨¡å‹è¿›è¡Œæ›´å‡†ç¡®çš„ä¿®æ­£
            response = ollama.chat(
                model=OLLAMA_MODEL_7B, 
                messages=[{
                    'role': 'user',
                    'content': prompt,
                    'images': [str(image_path)]
                }],
                options={'temperature': 0.0}
            )
            return self.clean_llm_output(response['message']['content'])
        except Exception as e:
            print(f"  âŒ Inference Error: {e}")
            return "ERROR"

    def find_image(self, filename, roi_id):
        """æŸ¥æ‰¾å›¾ç‰‡"""
        folder = os.path.splitext(filename)[0]
        # ç›´æ¥åœ¨ crops_base ä¸‹æŸ¥æ‰¾æ–‡ä»¶å¤¹ (å› ä¸º config ä¸­å·²æŒ‡å®šåˆ°å…·ä½“æ—¥æœŸç›®å½•)
        for ext in ['jpg', 'png']:
            p = self.crops_base / folder / f"{roi_id}.{ext}"
            if p.exists(): return p
        return None

    def process_log(self, log_path):
        filename = log_path.name
        print(f"\nğŸ”§ Correcting Abnormalities: {filename}")
        
        try:
            df_bad = pd.read_csv(log_path)
            if df_bad.empty: return

            # åŠ è½½å¯¹åº”çš„ Cleaned CSV ä»¥è®¡ç®— Context
            base_name = filename.replace("_Abnormal_Log.csv", "")
            cleaned_path = self.cleaned_dir / f"{base_name}_Cleaned.csv"
            medians = {}
            if cleaned_path.exists():
                medians = self.calculate_medians(cleaned_path)
            
            # æ–­ç‚¹ç»­ä¼ : æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„è¿›åº¦
            out_name = filename.replace(".csv", "_AI_Fixed.csv")
            out_path = self.output_dir / out_name
            
            if out_path.exists():
                df_progress = pd.read_csv(out_path)
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¡Œéƒ½å·²å¤„ç†å®Œæˆ
                if 'AI_Fixed' in df_progress.columns:
                    # ç»Ÿè®¡æœªå¤„ç†çš„è¡Œæ•° (AI_Fixed ä¸ºç©ºæˆ– NaN)
                    unprocessed_mask = df_progress['AI_Fixed'].isna() | (df_progress['AI_Fixed'] == "")
                    unprocessed_count = unprocessed_mask.sum()
                    
                    if unprocessed_count == 0:
                        print(f"  â­ï¸  Skipped (all {len(df_progress)} items already processed)")
                        return
                    
                    print(f"  ğŸ“‚ Resuming from checkpoint: {len(df_progress) - unprocessed_count}/{len(df_progress)} done")
                    df_bad = df_progress
                else:
                    df_bad['AI_Fixed'] = ""
            else:
                df_bad['AI_Fixed'] = ""
            
            # å¤„ç†è®¡æ•°å™¨ (ç”¨äºå®šæœŸä¿å­˜)
            save_interval = 5  # æ¯å¤„ç†5æ¡ä¿å­˜ä¸€æ¬¡
            processed_since_save = 0
            
            for idx, row in df_bad.iterrows():
                # æ–­ç‚¹ç»­ä¼ : è·³è¿‡å·²å¤„ç†çš„è¡Œ
                existing_val = row['AI_Fixed'] if 'AI_Fixed' in row.index else ""
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†: éç©ºã€éNaNã€éERROR
                if pd.notna(existing_val) and str(existing_val).strip() not in ["", "ERROR"]:
                    continue
                
                roi_id = row['ROI_ID']
                img_path = self.find_image(row['Filename'], roi_id)
                
                if not img_path:
                    df_bad.at[idx, 'AI_Fixed'] = "Image Not Found"
                    processed_since_save += 1
                else:
                    curr_median = medians.get(roi_id, None)
                    fixed_val = self.run_inference(img_path, roi_id, curr_median, row['Value'])
                    
                    # è®¡ç®—å½“å‰è¿›åº¦
                    done_count = (df_bad['AI_Fixed'].notna() & (df_bad['AI_Fixed'] != "")).sum() + 1
                    print(f"  [{done_count}/{len(df_bad)}] {roi_id}: {row['Value']} â†’ {fixed_val}")
                    df_bad.at[idx, 'AI_Fixed'] = fixed_val
                    processed_since_save += 1
                
                # å®šæœŸä¿å­˜ checkpoint
                if processed_since_save >= save_interval:
                    df_bad.to_csv(out_path, index=False)
                    processed_since_save = 0
            
            # æœ€ç»ˆä¿å­˜
            df_bad.to_csv(out_path, index=False)
            print(f"  âœ… Saved corrections to {out_name}")
            
        except Exception as e:
            print(f"  âŒ Error processing log: {e}")

    def run(self):
        print("\n" + "="*60)
        print(f"STAGE 2: Model Correction (Using {OLLAMA_MODEL_7B})")
        print("="*60)
        # æŸ¥æ‰¾ç”± Stage 1 ç”Ÿæˆçš„ Abnormal Logs
        logs = list(self.cleaned_dir.glob("*_Abnormal_Log.csv"))
        if not logs:
            print("âœ… No abnormalities to correct.")
            return
        
        print(f"Found {len(logs)} logs to process.\n")
        for log in logs:
            self.process_log(log)

# ================= é˜¶æ®µ3: åˆå¹¶ç»“æœ =================
class Stage3_Merge:
    """é˜¶æ®µ3: åˆå¹¶"""
    def __init__(self, cleaned_dir, fixed_dir, output_dir):
        self.cleaned_dir = Path(cleaned_dir)
        self.fixed_dir = Path(fixed_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run(self):
        print("\n" + "="*60)
        print("STAGE 3: Merging Results")
        print("="*60)
        
        fixed_logs = list(self.fixed_dir.glob("*_AI_Fixed.csv"))
        if not fixed_logs:
            print("No corrections to merge.")
            return

        for log in fixed_logs:
            base_name = log.name.replace("_Abnormal_Log_AI_Fixed.csv", "")
            cleaned_csv = self.cleaned_dir / f"{base_name}_Cleaned.csv"
            
            if not cleaned_csv.exists(): continue
            
            # æ–­ç‚¹ç»­ä¼ : æ£€æŸ¥æ˜¯å¦å·²åˆå¹¶
            out_path = self.output_dir / f"{base_name}_Final.csv"
            if out_path.exists():
                print(f"â­ï¸  Skipped merge: {base_name} (already exists: {out_path.name})")
                continue
            
            print(f"ğŸ”€ Merging: {base_name}")
            df_clean = pd.read_csv(cleaned_csv)
            df_fixed = pd.read_csv(log)
            
            count = 0
            for _, row in df_fixed.iterrows():
                val = row['AI_Fixed']
                if pd.isna(val) or val in ["ERROR", "Image Not Found"]: continue
                
                # æŸ¥æ‰¾å¹¶æ›´æ–°
                mask = df_clean['Filename'] == row['Filename']
                if mask.any():
                    df_clean.loc[mask, row['ROI_ID']] = val
                    count += 1
            
            df_clean.to_csv(out_path, index=False)
            print(f"  âœ… Updated {count} values â†’ {out_path.name}")

# ================= ä¸»ç¨‹åº =================
def main():
    # 1. è·¯å¾„é…ç½® (æ¥è‡ª Configï¼Œä½†åœ¨è¿™é‡Œå…·ä½“åŒ–)
    # BATCH_NAME å’Œ CROP_DIR_NAME å·²ç»åœ¨ config_pipeline.py ä¸­å®šä¹‰å¥½äº†
    # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨ config ä¸­çš„ OUTPUT_BASE ä¸‹çš„ç›®å½•
    
    # è¾“å…¥CSV: å¦‚æœæœ‰ 'CSV_Results' å­ç›®å½•åˆ™ç”¨ä¹‹ï¼Œå¦åˆ™ç”¨ stage1 æ ¹ç›®å½•
    CSV_SOURCE = STAGE_1_OCR / "CSV_Results"
    if not CSV_SOURCE.exists():
        CSV_SOURCE = STAGE_1_OCR
        
    CROPS_SOURCE = DEBUG_CROPS_BASE  # æ¥è‡ª config, å¯¹åº” 12-22-2025
    
    # é˜¶æ®µ 2 è¾“å‡ºç›®å½•
    STAGE2_OUT = STAGE_2_CLEANED
    # é˜¶æ®µ 3 è¾“å‡ºç›®å½•
    STAGE3_OUT = STAGE_3_3B_CORRECTED
    
    print("\n" + "="*80)
    print("ğŸš€ AUTOMATED DATA PIPELINE START")
    print(f"ğŸ“‚ CSV Source:   {CSV_SOURCE}")
    print(f"ğŸ–¼ï¸  Crops Source: {CROPS_SOURCE}")
    print(f"ğŸ¤– Model:        {OLLAMA_MODEL_7B}")  # ä½¿ç”¨ 7B è¿›è¡Œä¿®æ­£
    print("="*80)
    
    if not CSV_SOURCE.exists():
        print(f"âŒ Error: Input directory {CSV_SOURCE} does not exist!")
        return

    # --- Step 1: Validate ---
    s1 = Stage1_DataCleaning(CSV_SOURCE, STAGE2_OUT, CROPS_SOURCE)
    s1.run()
    
    # --- Step 2: Correct ---
    # æ³¨æ„ï¼šStage 2 è¯»å– Stage 1 è¾“å‡ºçš„ Log
    s2 = Stage2_Correction(STAGE2_OUT, CROPS_SOURCE, STAGE3_OUT)
    s2.run()
    
    # --- Step 3: Merge ---
    # æ³¨æ„ï¼šStage 3 è¯»å– Stage 2 è¾“å‡ºçš„ Fixed Log å’Œ Stage 1 çš„ Cleaned CSV
    s3 = Stage3_Merge(STAGE2_OUT, STAGE3_OUT, STAGE3_OUT)
    s3.run()
    
    print("\nâœ… PIPELINE FINISHED SUCCESSFULLY")

if __name__ == "__main__":
    main()